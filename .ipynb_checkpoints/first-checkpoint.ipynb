{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(file_id):\n",
    "    try:\n",
    "        with open(\"Dataset/\"+file_id+\".txt\", 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        culture_data = []\n",
    "        history_data = []\n",
    "        description_data = []\n",
    "        # Parse the lines and extract data\n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line in (\"Culture\", \"History\", \"Description\"):\n",
    "                current_section = line\n",
    "            else:\n",
    "                if current_section == \"Culture\":\n",
    "                    culture_data.append(line)\n",
    "                elif current_section == \"History\":\n",
    "                    history_data.append(line)\n",
    "                elif current_section == \"Description\":\n",
    "                    description_data.append(line)\n",
    "            culture_text = \" \".join(culture_data)\n",
    "            history_text = \" \".join(history_data)\n",
    "            description_text = \" \".join(description_data)\n",
    "        return culture_text , history_text , description_text\n",
    "    except:\n",
    "        return \"\" ,\"\",\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset.drop(['Review'],axis=1,inplace=True)\n",
    "dataset[[\"History\", \"Culture\", \"Description\"]] = dataset.apply(lambda row: read_file(str(row['Index'])), axis='columns', result_type='expand')\n",
    "# dataset.set_index('Index',inplace=True)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clmns = [\"name\",\"location\",\"description\",\"history\",\"culture\",\"image_url\"]\n",
    "# temp =[[\"Taj Mahal\",\"Agra\",\"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra.\",\"Built by Mughal Emperor Shah Jahan in memory of his wife Mumtaz Mahal.\",\"A symbol of love and a UNESCO World Heritage Site.\",\"https://example.com/taj_mahal.jpg\"],\n",
    "# [\"Great Wall of China\",\"China\",\"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials.\",\"Built along the northern borders of China to protect against invasions.\",\"A UNESCO World Heritage Site and one of the most impressive architectural feats in history.\",\"https://example.com/great_wall.jpg\"]]\n",
    "\n",
    "# df = pd.DataFrame( temp , columns=clmns )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    preprocessed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in string.punctuation and token not in stop_words:\n",
    "            # stemmed_token = stemmer.stem(token)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token)\n",
    "            preprocessed_tokens.append(lemmatized_token)\n",
    "    preprocessed_text = ' '.join(preprocessed_tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in dataset.iterrows():\n",
    "    place_id = row['Index'] # Assuming the row index as the unique identifier\n",
    "    name = row['Name']\n",
    "    location = row['Location']\n",
    "    description = row['Description']\n",
    "    history = row['History']\n",
    "    culture = row['Culture']\n",
    "    image_url = row['URL']\n",
    "\n",
    "    # print(place_id)\n",
    "    tokens = set(name.lower().split() + location.lower().split() + description.lower().split() + history.lower().split() + culture.lower().split())\n",
    "    # if place_id==10005:\n",
    "    #     print(name,name.lower().split())\n",
    "    #     print(description)\n",
    "    #     print(description.lower().split())\n",
    "    #     print(culture)\n",
    "    #     print(culture.lower().split())\n",
    "    #     print(tokens)\n",
    "\n",
    "\n",
    "    for token in tokens:\n",
    "        index[token].append(place_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = input(\"Enter input\")\n",
    "query_tokens = query.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{10005}]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "matching_place_ids = [set(index[query_token]) for query_token in query_tokens]\n",
    "matching_place_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[296], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matching_place_ids \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_tokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m result_place_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;241m*\u001b[39mmatching_place_ids)\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m dataset[dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(result_place_ids)]\n",
      "Cell \u001b[0;32mIn[296], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matching_place_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mset\u001b[39m(\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_token\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m query_token \u001b[38;5;129;01min\u001b[39;00m query_tokens]\n\u001b[1;32m      2\u001b[0m result_place_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;241m*\u001b[39mmatching_place_ids)\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m dataset[dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(result_place_ids)]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "matching_place_ids = [set(index[query_token]) for query_token in query_tokens]\n",
    "result_place_ids = set.intersection(*matching_place_ids)\n",
    "\n",
    "results = dataset[dataset['Index'].isin(result_place_ids)]\n",
    "\n",
    "\n",
    "cnts = results.shape[0]\n",
    "\n",
    "print(\"There are following\",str(cnts),\"results:-\" )\n",
    "\n",
    "\n",
    "\n",
    "for index, row in result.iterrows():\n",
    "    print(row['Name'])\n",
    "    print(row['Location'])\n",
    "    print(\"Description:-\")\n",
    "    print(row['Description'])\n",
    "    print(\"History\")\n",
    "    print(row['History'])\n",
    "    print(\"Culture:\")\n",
    "    print(\"Related Images:\")\n",
    "    print(row['URL'])\n",
    "    print(\"---\"*30)\n",
    "    print(\"   \"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
